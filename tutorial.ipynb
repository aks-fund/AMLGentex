{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMLGentex Framework Tutorial\n",
    "\n",
    "This notebook demonstrates the complete AMLGentex workflow for synthetic AML detection data generation and federated learning.\n",
    "\n",
    "## Overview\n",
    "\n",
    "AMLGentex is a framework for:\n",
    "1. **Generating** synthetic AML transaction data with configurable patterns\n",
    "2. **Preprocessing** transactions into ML-ready features\n",
    "3. **Training** ML models in three settings:\n",
    "   - **Centralized**: All data combined\n",
    "   - **Federated**: Privacy-preserving collaborative learning\n",
    "   - **Isolated**: Each institution trains independently\n",
    "4. **Visualizing** results and transaction networks\n",
    "\n",
    "## Convention Over Configuration\n",
    "\n",
    "The framework follows a **convention-over-configuration** approach:\n",
    "- Paths are auto-discovered from experiment name\n",
    "- Clients (banks) are auto-discovered from data\n",
    "- Results follow standard directory structure\n",
    "\n",
    "**Standard experiment structure:**\n",
    "```\n",
    "experiments/my_experiment/\n",
    "├── config/\n",
    "│   ├── data.yaml              # Data generation config\n",
    "│   ├── preprocessing.yaml     # Feature engineering config\n",
    "│   └── models.yaml            # Model training config\n",
    "├── spatial/                   # Generated spatial graph\n",
    "├── temporal/                  # Generated transactions\n",
    "├── preprocessed/              # ML-ready features\n",
    "│   ├── centralized/           # Combined data\n",
    "│   └── clients/               # Per-bank data\n",
    "├── results/                   # Training results\n",
    "│   ├── centralized/\n",
    "│   ├── federated/\n",
    "│   └── isolated/\n",
    "└── visualizations/            # Plots and analysis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Set experiment name - this is the ONLY thing you need to configure!\n",
    "EXPERIMENT = \"tutorial_demo\"\n",
    "experiment_root = project_root / \"experiments\" / EXPERIMENT\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Experiment: {EXPERIMENT}\")\n",
    "print(f\"Experiment root: {experiment_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Experiment Configuration\n",
    "\n",
    "We'll create a minimal experiment with 1,000 accounts and 2 banks.\n",
    "\n",
    "### 1.1 Create Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment directories\n",
    "config_dir = experiment_root / \"config\"\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "print(f\"✓ Created experiment structure at: {experiment_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create Data Generation Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of creating from scratch, copy the working config from template\n",
    "import shutil\n",
    "\n",
    "config_files = ['data.yaml',\n",
    "                'preprocessing.yaml', \n",
    "                'models.yaml']\n",
    "\n",
    "template_config = project_root / \"experiments\" / \"template_experiment\" / \"config\"\n",
    "\n",
    "if template_config.exists():\n",
    "    # Copy the config files\n",
    "    for f in config_files:\n",
    "        shutil.copy(template_config / f, config_dir / f)\n",
    "        print(f\"Copied {f} from {template_config}\")\n",
    "    \n",
    "    # Update simulation_name in data.yaml to match experiment name\n",
    "    data_yaml_path = config_dir / 'data.yaml'\n",
    "    with open(data_yaml_path, 'r') as f:\n",
    "        data_config = yaml.safe_load(f)\n",
    "    \n",
    "    data_config['general']['simulation_name'] = EXPERIMENT\n",
    "    \n",
    "    with open(data_yaml_path, 'w') as f:\n",
    "        yaml.dump(data_config, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    print(f\"\\n✓ Updated simulation_name to: {EXPERIMENT}\")\n",
    "else:\n",
    "    print(\"Template config not found. Please ensure template_experiment exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create Supporting Configuration Files\n",
    "\n",
    "We'll copy minimal configuration files from the template experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use existing 10k_accounts config as a template for supporting files\n",
    "template_dir = project_root / \"experiments\" / \"template_experiment\" / \"config\"\n",
    "\n",
    "# Copy supporting files\n",
    "import shutil\n",
    "\n",
    "support_files = [\n",
    "    'accounts.csv',\n",
    "    'alertPatterns.csv', \n",
    "    'normalModels.csv',\n",
    "    'degree.csv',\n",
    "    'transactionType.csv'\n",
    "]\n",
    "\n",
    "for file in support_files:\n",
    "    src = template_dir / file\n",
    "    dst = config_dir / file\n",
    "    if src.exists():\n",
    "        shutil.copy(src, dst)\n",
    "        print(f\"✓ Copied {file}\")\n",
    "    else:\n",
    "        print(f\"⚠ Warning: {file} not found in template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Synthetic Transaction Data\n",
    "\n",
    "Now we'll generate synthetic AML transaction data with both normal and suspicious (SAR) patterns.\n",
    "\n",
    "**Convention**: Paths are automatically constructed from experiment name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_creation import DataGenerator\n",
    "from src.utils.config import load_data_config\n",
    "import tempfile\n",
    "\n",
    "# Load config with auto-discovered paths\n",
    "data_config = load_data_config(str(config_dir / 'data.yaml'))\n",
    "\n",
    "print(\"Configuration loaded with auto-discovered paths:\")\n",
    "print(f\"  Input dir: {data_config['input']['directory']}\")\n",
    "print(f\"  Output dir: {data_config['output']['directory']}\")\n",
    "print()\n",
    "\n",
    "# DataGenerator expects a config file path, so we create a temporary file\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as tmp:\n",
    "    yaml.dump(data_config, tmp, default_flow_style=False, sort_keys=False)\n",
    "    tmp_config = tmp.name\n",
    "\n",
    "try:\n",
    "    print(\"Generating synthetic transaction data...\")\n",
    "    print(\"This may take a few minutes...\\n\")\n",
    "    \n",
    "    generator = DataGenerator(tmp_config)\n",
    "    tx_log_file = generator()\n",
    "    \n",
    "    print(f\"\\n✓ Transaction data generated: {tx_log_file}\")\n",
    "finally:\n",
    "    os.unlink(tmp_config)\n",
    "\n",
    "# Load and inspect the generated data\n",
    "df = pd.read_parquet(tx_log_file)\n",
    "print(f\"\\nGenerated {len(df):,} transactions\")\n",
    "print(f\"Banks: {df['bankOrig'].unique().tolist()}\")\n",
    "print(f\"SAR transactions: {df['isSAR'].sum():,} ({df['isSAR'].mean()*100:.2f}%)\")\n",
    "print(f\"Time range: steps {df['step'].min()} to {df['step'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Optimize Data Generation (Optional)\n",
    "\n",
    "**This step is optional but recommended** - it uses Bayesian optimization to tune data generation parameters to achieve target model performance under operational constraints.\n",
    "\n",
    "The optimizer:\n",
    "- Adjusts parameters like SAR transaction amounts, spending patterns, and behavioral features\n",
    "- Trains a quick classifier to evaluate each configuration\n",
    "- Returns Pareto-optimal solutions balancing utility metric accuracy and feature stability\n",
    "\n",
    "### Three Optimization Objectives\n",
    "\n",
    "Choose one based on your operational scenario:\n",
    "\n",
    "**1. Precision@K (Alert Budget Constraint)**\n",
    "- Use when: Your team can investigate K alerts per day\n",
    "- Example: \"We can review 100 alerts daily. Optimize for 80% precision in top 100.\"\n",
    "- Parameters: `constraint_type='K'`, `constraint_value=100`, `utility_metric='precision'`, `target=0.8`\n",
    "\n",
    "**2. Recall at FPR≤α (Regulatory Constraint)**\n",
    "- Use when: Compliance requires FPR below a threshold\n",
    "- Example: \"FPR must be ≤1%. Optimize for 70% recall at this limit.\"\n",
    "- Parameters: `constraint_type='fpr'`, `constraint_value=0.01`, `utility_metric='recall'`, `target=0.7`\n",
    "\n",
    "**3. Precision at Recall≥target (Coverage Constraint)**\n",
    "- Use when: Must detect a minimum fraction of SARs\n",
    "- Example: \"Must detect 70% of SARs. Optimize for 60% precision at this recall.\"\n",
    "- Parameters: `constraint_type='recall'`, `constraint_value=0.7`, `utility_metric='precision'`, `target=0.6`\n",
    "\n",
    "### 2.1.1 Configure and Run Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_tuning import DataTuner\n",
    "from src.data_creation import DataGenerator\n",
    "from src.feature_engineering import DataPreprocessor\n",
    "from src.utils.config import load_data_config, load_preprocessing_config\n",
    "import yaml\n",
    "\n",
    "# ===== CONFIGURE YOUR OPTIMIZATION OBJECTIVE =====\n",
    "# Choose ONE of the three objectives below:\n",
    "\n",
    "# Option 1: Precision@K (Alert Budget)\n",
    "CONSTRAINT_TYPE = 'K'\n",
    "CONSTRAINT_VALUE = 100  # Top 100 alerts\n",
    "UTILITY_METRIC = 'precision'\n",
    "TARGET = 0.02  # only 2% are true positives out of the K reported ones\n",
    "\n",
    "#Option 2: Recall at FPR (Regulatory Constraint)\n",
    "# CONSTRAINT_TYPE = 'fpr'\n",
    "# CONSTRAINT_VALUE = 0.01  # FPR ≤ 1%\n",
    "# UTILITY_METRIC = 'recall'\n",
    "# TARGET = 0.02  # 2% recall\n",
    "\n",
    "# Option 3: Precision at Recall (Coverage Constraint)\n",
    "# CONSTRAINT_TYPE = 'recall'\n",
    "# CONSTRAINT_VALUE = 0.7  # Recall ≥ 70%\n",
    "# UTILITY_METRIC = 'precision'\n",
    "# TARGET = 0.02  # 2% precision\n",
    "\n",
    "N_TRIALS = 100  # Number of data optimization trials\n",
    "N_MODEL_TRIALS = 20  # Number of model hyperparameter trials per data configuration\n",
    "# ==================================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "if CONSTRAINT_TYPE == 'K':\n",
    "    print(f\"\\nObjective: Optimize {UTILITY_METRIC} in top {int(CONSTRAINT_VALUE)} reported alerts\")\n",
    "elif CONSTRAINT_TYPE == 'fpr':\n",
    "    print(f\"\\nObjective: Optimize {UTILITY_METRIC} at FPR ≤ {CONSTRAINT_VALUE}\")\n",
    "elif CONSTRAINT_TYPE == 'recall':\n",
    "    print(f\"\\nObjective: Optimize {UTILITY_METRIC} at Recall ≥ {CONSTRAINT_VALUE}\")\n",
    "print(f\"Target: {TARGET:.1%} {UTILITY_METRIC}\")\n",
    "print(f\"Data optimization trials: {N_TRIALS}\")\n",
    "print(f\"Model hyperparameter trials per data config: {N_MODEL_TRIALS}\")\n",
    "print()\n",
    "\n",
    "# Set up paths\n",
    "data_yaml_path = str(config_dir / 'data.yaml')\n",
    "\n",
    "# Load configs with auto-discovered paths\n",
    "data_config = load_data_config(data_yaml_path)\n",
    "preproc_config = load_preprocessing_config(str(config_dir / 'preprocessing.yaml'))\n",
    "\n",
    "# Write config back with absolute paths\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "# Load model config\n",
    "with open(config_dir / 'models.yaml', 'r') as f:\n",
    "    models_config = yaml.safe_load(f)\n",
    "\n",
    "# Set up optimization directory\n",
    "bo_dir = experiment_root / 'data_tuning'\n",
    "os.makedirs(bo_dir, exist_ok=True)\n",
    "\n",
    "# Create tuning config\n",
    "tuning_config = {\n",
    "    'preprocess': preproc_config.copy(),\n",
    "    **models_config\n",
    "}\n",
    "tuning_config['preprocess']['preprocessed_data_dir'] = str(bo_dir / 'preprocessed')\n",
    "tuning_config['DecisionTreeClassifier']['default']['device'] = 'cpu'\n",
    "\n",
    "print(f\"Optimizer will directly modify: {data_yaml_path}\\n\")\n",
    "\n",
    "# Initialize generator\n",
    "generator = DataGenerator(data_yaml_path, verbose=False)\n",
    "\n",
    "# Run spatial simulation once (reused across all trials)\n",
    "print(\"Running spatial simulation (graph generation) once...\")\n",
    "print(\"This graph will be reused across all optimization trials.\\n\")\n",
    "generator.run_spatial(force=False)\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor(preproc_config, verbose=False)\n",
    "\n",
    "# Initialize data tuner\n",
    "print(\"Initializing data optimizer...\")\n",
    "print(\"Each trial will:\")\n",
    "print(\"  1. Update data.yaml with new parameters\")\n",
    "print(\"  2. Regenerate temporal simulation (reuses spatial graph)\")\n",
    "print(\"  3. Preprocess into features\")\n",
    "print(f\"  4. Tune DecisionTreeClassifier hyperparameters ({N_MODEL_TRIALS} trials)\")\n",
    "print(f\"  5. Measure {UTILITY_METRIC} and feature stability\")\n",
    "print()\n",
    "\n",
    "tuner = DataTuner(\n",
    "    data_conf_file=data_yaml_path,\n",
    "    config=tuning_config,\n",
    "    generator=generator,\n",
    "    preprocessor=preprocessor,\n",
    "    target=TARGET,\n",
    "    constraint_type=CONSTRAINT_TYPE,\n",
    "    constraint_value=CONSTRAINT_VALUE,\n",
    "    utility_metric=UTILITY_METRIC,\n",
    "    model='DecisionTreeClassifier',\n",
    "    bo_dir=str(bo_dir),\n",
    "    seed=42,\n",
    "    num_trials_model=N_MODEL_TRIALS,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "print(f\"Starting Bayesian optimization ({N_TRIALS} data trials × {N_MODEL_TRIALS} model trials each)...\")\n",
    "print(\"This may take some time depending on data size and number of trials.\\n\")\n",
    "\n",
    "best_trials = tuner(n_trials=N_TRIALS)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMIZATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nFound {len(best_trials)} Pareto-optimal solutions\")\n",
    "print(f\"Results saved to: {bo_dir}\")\n",
    "print(f\"  - pareto_front.png: Visualization of trade-offs\")\n",
    "print(f\"  - best_trials.txt: Parameter values for each solution\")\n",
    "print(f\"  - data_tuning_study.db: Full optimization history\")\n",
    "print(f\"\\nNote: {data_yaml_path} now contains parameters from the last trial.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Visualize Pareto Front and Select Solution\n",
    "\n",
    "The optimizer returns multiple Pareto-optimal solutions. Each represents a different trade-off:\n",
    "- **Low FPR loss** = data closely matches target FPR\n",
    "- **Low feature importance loss** = more stable/consistent features\n",
    "\n",
    "Let's visualize the results and select the best trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Pareto front\n",
    "from IPython.display import Image, display\n",
    "\n",
    "pareto_front_path = bo_dir / 'pareto_front.png'\n",
    "if pareto_front_path.exists():\n",
    "    display(Image(filename=str(pareto_front_path)))\n",
    "else:\n",
    "    print(\"Pareto front plot not found\")\n",
    "\n",
    "# Show best trials\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PARETO-OPTIMAL SOLUTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, trial in enumerate(best_trials):\n",
    "    utility_loss = trial.values[0]\n",
    "    importance_loss = trial.values[1]\n",
    "    \n",
    "    # Get achieved utility from trial attributes\n",
    "    achieved_utility = trial.user_attrs.get('utility_metric', TARGET)\n",
    "    \n",
    "    print(f\"\\nTrial {trial.number} (Solution #{i+1}):\")\n",
    "    print(f\"  {UTILITY_METRIC.capitalize()}: {achieved_utility:.4f} (target: {TARGET:.4f}, loss: {utility_loss:.4f})\")\n",
    "    print(f\"  Feature stability loss: {importance_loss:.4f}\")\n",
    "    print(f\"  Parameters:\")\n",
    "    for param, value in trial.params.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"    {param}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"    {param}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nTo select a trial:\")\n",
    "print(f\"  1. Low {UTILITY_METRIC} loss = better match to target\")\n",
    "print(\"  2. Low importance loss = more stable features\")\n",
    "print(\"  3. Balance both for general use\")\n",
    "print(f\"\\nTrial {best_trials[0].number} has the best balance\")\n",
    "print(f\"Run next cell to apply it (or choose a different trial number)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Apply Selected Trial and Regenerate Data\n",
    "\n",
    "Choose which trial to use and regenerate the data with optimized parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SELECT WHICH TRIAL TO USE =====\n",
    "SELECTED_TRIAL = best_trials[0].number  # Default: use first (best balance)\n",
    "# You can change this to any trial number from the list above\n",
    "# ======================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"APPLYING OPTIMIZED PARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Update data.yaml with selected trial's parameters\n",
    "tuner.optimizer.update_config_with_trial(SELECTED_TRIAL)\n",
    "\n",
    "# Regenerate data with optimized parameters\n",
    "print(\"\\nRegenerating data with optimized parameters...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "# Reload config and regenerate\n",
    "data_config_optimized = load_data_config(data_yaml_path)\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    yaml.dump(data_config_optimized, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "# Reinitialize generator with updated config\n",
    "generator = DataGenerator(data_yaml_path)\n",
    "optimized_tx_log = generator()\n",
    "\n",
    "print(f\"\\n✓ Optimized transaction data generated: {optimized_tx_log}\")\n",
    "\n",
    "# Load and inspect the optimized data\n",
    "df_optimized = pd.read_parquet(optimized_tx_log)\n",
    "print(f\"\\nGenerated {len(df_optimized):,} transactions\")\n",
    "print(f\"Banks: {df_optimized['bankOrig'].unique().tolist()}\")\n",
    "print(f\"SAR transactions: {df_optimized['isSAR'].sum():,} ({df_optimized['isSAR'].mean()*100:.2f}%)\")\n",
    "print(f\"Time range: steps {df_optimized['step'].min()} to {df_optimized['step'].max()}\")\n",
    "\n",
    "# Compare with original (if available)\n",
    "if 'df' in globals():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COMPARISON: Original vs Optimized\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Total transactions: {len(df):,} → {len(df_optimized):,}\")\n",
    "    print(f\"  SAR rate: {df['isSAR'].mean()*100:.2f}% → {df_optimized['isSAR'].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n✅ Data optimization complete!\")\n",
    "if CONSTRAINT_TYPE == 'K':\n",
    "    print(f\"The optimized data should achieve ~{TARGET:.1%} {UTILITY_METRIC} in top {int(CONSTRAINT_VALUE)} alerts.\")\n",
    "elif CONSTRAINT_TYPE == 'fpr':\n",
    "    print(f\"The optimized data should achieve ~{TARGET:.1%} {UTILITY_METRIC} at FPR ≤ {CONSTRAINT_VALUE}.\")\n",
    "elif CONSTRAINT_TYPE == 'recall':\n",
    "    print(f\"The optimized data should achieve ~{TARGET:.1%} {UTILITY_METRIC} at Recall ≥ {CONSTRAINT_VALUE}.\")\n",
    "print(f\"\\nProceed to Step 3 to preprocess this optimized data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Note about Step 2.1:**\n",
    "- **If you ran optimization**: The data has been regenerated with optimized parameters. Proceed to Step 3.\n",
    "- **If you skipped optimization**: You can proceed directly to Step 3 with the original data from Step 2.\n",
    "\n",
    "The optimization step is particularly useful when:\n",
    "- **Precision@K**: You have a fixed alert review budget and need high precision\n",
    "- **Recall at FPR**: Regulatory requirements constrain false positive rates\n",
    "- **Precision at Recall**: You must maintain minimum SAR detection coverage\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess Data for ML\n",
    "\n",
    "Convert raw transactions into ML-ready features with graph structure.\n",
    "\n",
    "**Convention**: Creates both centralized and per-client datasets automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_engineering import DataPreprocessor, summarize_dataset\n",
    "from src.utils.config import load_preprocessing_config\n",
    "\n",
    "# Copy the preprocessing.yaml from template\n",
    "template_dir = project_root / \"experiments\" / \"template_experiment\" / \"config\"\n",
    "\n",
    "# Load config with auto-discovered paths\n",
    "preproc_config = load_preprocessing_config(str(config_dir / 'preprocessing.yaml'))\n",
    "\n",
    "print(\"Preprocessing configuration:\")\n",
    "print(f\"  Raw data: {preproc_config['raw_data_file']}\")\n",
    "print(f\"  Output dir: {preproc_config['preprocessed_data_dir']}\")\n",
    "print()\n",
    "\n",
    "print(\"Preprocessing transactions...\")\n",
    "print(\"Generating temporal features with rolling windows...\\n\")\n",
    "\n",
    "preprocessor = DataPreprocessor(preproc_config)\n",
    "datasets = preprocessor(preproc_config['raw_data_file'])\n",
    "\n",
    "# Create directory structure\n",
    "preprocessed_dir = Path(preproc_config['preprocessed_data_dir'])\n",
    "os.makedirs(preprocessed_dir / 'centralized', exist_ok=True)\n",
    "os.makedirs(preprocessed_dir / 'clients', exist_ok=True)\n",
    "\n",
    "# Save centralized datasets\n",
    "for name, dataset in datasets.items():\n",
    "    dataset.to_parquet(preprocessed_dir / 'centralized' / f'{name}.parquet', index=False)\n",
    "    print(f\"  Saved centralized/{name}.parquet: {len(dataset):,} samples\")\n",
    "\n",
    "# Split by bank and save per-client datasets\n",
    "banks = datasets['trainset_nodes']['bank'].unique()\n",
    "print(f\"\\nSplitting data for {len(banks)} banks: {banks.tolist()}\")\n",
    "\n",
    "for bank in banks:\n",
    "    bank_str = str(bank)\n",
    "    bank_dir = preprocessed_dir / 'clients' / bank_str\n",
    "    os.makedirs(bank_dir, exist_ok=True)\n",
    "    \n",
    "    # Save nodes\n",
    "    for split in ['trainset', 'valset', 'testset']:\n",
    "        df_nodes = datasets[f'{split}_nodes']\n",
    "        df_nodes[df_nodes['bank'] == bank].to_parquet(\n",
    "            bank_dir / f'{split}_nodes.parquet', index=False\n",
    "        )\n",
    "    \n",
    "    # Save edges (only intra-bank transactions)\n",
    "    if 'trainset_edges' in datasets:\n",
    "        for split in ['trainset', 'valset', 'testset']:\n",
    "            unique_nodes = df_nodes[df_nodes['bank'] == bank]['account'].unique()\n",
    "            df_edges = datasets[f'{split}_edges']\n",
    "            df_edges[\n",
    "                (df_edges['src'].isin(unique_nodes)) & \n",
    "                (df_edges['dst'].isin(unique_nodes))\n",
    "            ].to_parquet(bank_dir / f'{split}_edges.parquet', index=False)\n",
    "    \n",
    "    print(f\"  ✓ Saved data for bank: {bank_str}\")\n",
    "\n",
    "# Generate summary statistics\n",
    "print(\"\\nGenerating dataset summary...\")\n",
    "summarize_dataset(\n",
    "    str(preprocessed_dir),\n",
    "    raw_data_file=preproc_config['raw_data_file']\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train ML Models\n",
    "\n",
    "Train a Graph Neural Network (GraphSAGE) in three different settings.\n",
    "\n",
    "**Convention**: All paths and clients are auto-discovered!\n",
    "\n",
    "### 4.1 Centralized Training\n",
    "\n",
    "Train on all data combined (baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.training.centralized import centralized\n",
    "from src.ml import clients, models\n",
    "from src.utils.config import load_training_config\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CENTRALIZED TRAINING - GraphSAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# NOTE: Even if models.yaml contains multiple model configurations,\n",
    "# we only load GraphSAGE here by specifying it as the model_type parameter\n",
    "config = load_training_config(\n",
    "    str(config_dir / 'models.yaml'),\n",
    "    'GraphSAGE',  # Selects only GraphSAGE config from models.yaml\n",
    "    setting='centralized',\n",
    "    client_type='TorchGeometricClient'\n",
    ")\n",
    "\n",
    "# Force CPU device (override config if needed)\n",
    "config['device'] = 'cpu'\n",
    "\n",
    "print(f\"\\nTraining GraphSAGE on centralized data:\")\n",
    "print(f\"  Device: {config['device']}\")\n",
    "print(f\"  Nodes: {config['trainset_nodes']}\")\n",
    "print(f\"  Edges: {config['trainset_edges']}\")\n",
    "print()\n",
    "\n",
    "# NOTE: input_dim is automatically detected from the data by the client\n",
    "# No need to manually calculate it!\n",
    "\n",
    "# Train GraphSAGE model\n",
    "Client = getattr(clients, 'TorchGeometricClient')\n",
    "Model = getattr(models, 'GraphSAGE')\n",
    "\n",
    "results_centralized = centralized(\n",
    "    seed=42,\n",
    "    Client=Client,\n",
    "    Model=Model,\n",
    "    **config\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results_dir = experiment_root / 'results' / 'centralized' / 'GraphSAGE'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "with open(results_dir / 'results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_centralized, f)\n",
    "\n",
    "# Display final metrics\n",
    "for client_id, metrics in results_centralized.items():\n",
    "    print(f\"\\n{client_id} - Final Metrics:\")\n",
    "    print(f\"  Test Average Precision: {metrics['testset']['average_precision'][-1]:.4f}\")\n",
    "    print(f\"  Test F1: {metrics['testset']['f1'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Federated Training\n",
    "\n",
    "Privacy-preserving collaborative learning across banks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.training.federated import federated\n",
    "from src.ml import servers\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FEDERATED TRAINING - GraphSAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# NOTE: Even if models.yaml contains multiple model configurations,\n",
    "# we only load GraphSAGE here by specifying it as the model_type parameter\n",
    "config = load_training_config(\n",
    "    str(config_dir / 'models.yaml'),\n",
    "    'GraphSAGE',  # Selects only GraphSAGE config from models.yaml\n",
    "    setting='federated',\n",
    "    client_type='TorchGeometricClient'\n",
    ")\n",
    "\n",
    "# Force CPU device (override config if needed)\n",
    "config['device'] = 'cpu'\n",
    "\n",
    "print(f\"\\nAuto-discovered clients: {list(config['clients'].keys())}\")\n",
    "for client_id, client_config in config['clients'].items():\n",
    "    print(f\"  {client_id}:\")\n",
    "    print(f\"    Nodes: {client_config['trainset_nodes']}\")\n",
    "\n",
    "print(f\"\\nDevice: {config['device']}\")\n",
    "print()\n",
    "\n",
    "# NOTE: input_dim is automatically detected from the data by the client\n",
    "# No need to manually calculate it!\n",
    "\n",
    "# Train GraphSAGE model with federated learning\n",
    "Server = getattr(servers, 'TorchServer')\n",
    "Client = getattr(clients, 'TorchGeometricClient')\n",
    "Model = getattr(models, 'GraphSAGE')\n",
    "\n",
    "results_federated = federated(\n",
    "    seed=42,\n",
    "    Server=Server,\n",
    "    Client=Client,\n",
    "    Model=Model,\n",
    "    n_workers=2,\n",
    "    **config\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results_dir = experiment_root / 'results' / 'federated' / 'GraphSAGE'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "with open(results_dir / 'results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_federated, f)\n",
    "\n",
    "# Display final metrics\n",
    "print(\"\\nFinal Metrics (per client):\")\n",
    "for client_id, metrics in results_federated.items():\n",
    "    print(f\"\\n{client_id}:\")\n",
    "    print(f\"  Test Average Precision: {metrics['testset']['average_precision'][-1]:.4f}\")\n",
    "    print(f\"  Test F1: {metrics['testset']['f1'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Isolated Training\n",
    "\n",
    "Each bank trains independently on their own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.training.isolated import isolated\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ISOLATED TRAINING - GraphSAGE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# NOTE: Even if models.yaml contains multiple model configurations,\n",
    "# we only load GraphSAGE here by specifying it as the model_type parameter\n",
    "config = load_training_config(\n",
    "    str(config_dir / 'models.yaml'),\n",
    "    'GraphSAGE',  # Selects only GraphSAGE config from models.yaml\n",
    "    setting='isolated',\n",
    "    client_type='TorchGeometricClient'\n",
    ")\n",
    "\n",
    "# Force CPU device (override config if needed)\n",
    "config['device'] = 'cpu'\n",
    "\n",
    "print(f\"\\nTraining isolated GraphSAGE models for: {list(config['clients'].keys())}\")\n",
    "print(f\"Device: {config['device']}\")\n",
    "print()\n",
    "\n",
    "# NOTE: input_dim is automatically detected from the data by the client\n",
    "# No need to manually calculate it!\n",
    "\n",
    "# Train GraphSAGE models independently\n",
    "Client = getattr(clients, 'TorchGeometricClient')\n",
    "Model = getattr(models, 'GraphSAGE')\n",
    "\n",
    "results_isolated = isolated(\n",
    "    seed=42,\n",
    "    Client=Client,\n",
    "    Model=Model,\n",
    "    n_workers=2,\n",
    "    **config\n",
    ")\n",
    "\n",
    "# Save results\n",
    "results_dir = experiment_root / 'results' / 'isolated' / 'GraphSAGE'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "with open(results_dir / 'results.pkl', 'wb') as f:\n",
    "    pickle.dump(results_isolated, f)\n",
    "\n",
    "# Display final metrics\n",
    "print(\"\\nFinal Metrics (per client):\")\n",
    "for client_id, metrics in results_isolated.items():\n",
    "    print(f\"\\n{client_id}:\")\n",
    "    print(f\"  Test Average Precision: {metrics['testset']['average_precision'][-1]:.4f}\")\n",
    "    print(f\"  Test F1: {metrics['testset']['f1'][-1]:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Results saved to: {results_dir}\")\n",
    "print(\"\\n✅ All training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Results\n",
    "\n",
    "Compare performance across the three training settings.\n",
    "\n",
    "### 5.1 Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualize import plot_metrics\n",
    "from src.visualize.utils import discover_results, load_results\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Auto-discover all results\n",
    "results_files = discover_results(experiment_root)\n",
    "\n",
    "print(f\"\\nFound {len(results_files)} results file(s):\")\n",
    "for key in results_files:\n",
    "    print(f\"  - {key}\")\n",
    "\n",
    "# Generate plots for each\n",
    "print(\"\\nGenerating plots...\")\n",
    "for key, results_file in results_files.items():\n",
    "    print(f\"  Processing {key}...\")\n",
    "    data = load_results(results_file)\n",
    "    output_dir = results_file.parent\n",
    "    \n",
    "    plot_metrics(\n",
    "        data,\n",
    "        str(output_dir),\n",
    "        metrics=['average_precision', 'f1', 'loss'],\n",
    "        clients=None,\n",
    "        datasets=['trainset', 'valset', 'testset'],\n",
    "        reduction='mean',\n",
    "        formats=['png']\n",
    "    )\n",
    "    print(f\"    Saved plots to: {output_dir}/png/\")\n",
    "\n",
    "print(\"\\n✓ All plots generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all results and compare\n",
    "import numpy as np\n",
    "\n",
    "comparison = {}\n",
    "\n",
    "for setting in ['centralized', 'federated', 'isolated']:\n",
    "    results_file = experiment_root / 'results' / setting / 'GraphSAGE' / 'results.pkl'\n",
    "    if results_file.exists():\n",
    "        with open(results_file, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "        \n",
    "        # Average across clients\n",
    "        avg_ap = np.mean([r['testset']['average_precision'][-1] for r in results.values()])\n",
    "        avg_f1 = np.mean([r['testset']['f1'][-1] for r in results.values()])\n",
    "        \n",
    "        comparison[setting] = {\n",
    "            'Average Precision': avg_ap,\n",
    "            'F1 Score': avg_f1\n",
    "        }\n",
    "\n",
    "# Display comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n{'Setting':<15} {'Avg Precision':<15} {'F1 Score':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for setting, metrics in comparison.items():\n",
    "    print(f\"{setting.capitalize():<15} {metrics['Average Precision']:<15.4f} {metrics['F1 Score']:<15.4f}\")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "settings = list(comparison.keys())\n",
    "ap_scores = [comparison[s]['Average Precision'] for s in settings]\n",
    "f1_scores = [comparison[s]['F1 Score'] for s in settings]\n",
    "\n",
    "axes[0].bar(settings, ap_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[0].set_ylabel('Average Precision')\n",
    "axes[0].set_title('Average Precision by Training Setting')\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "axes[1].bar(settings, f1_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
    "axes[1].set_ylabel('F1 Score')\n",
    "axes[1].set_title('F1 Score by Training Setting')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save comparison plot\n",
    "comparison_dir = experiment_root / 'visualizations' / 'comparison'\n",
    "os.makedirs(comparison_dir, exist_ok=True)\n",
    "plt.savefig(comparison_dir / 'setting_comparison.png', dpi=150)\n",
    "print(f\"\\n✓ Comparison plot saved to: {comparison_dir / 'setting_comparison.png'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Interactive Network Explorer Dashboard\n",
    "\n",
    "For interactive exploration of the data with filtering by banks, transaction models, and time ranges, use the standalone dashboard:\n",
    "\n",
    "```bash\n",
    "# Install network-explorer dependencies if not already installed\n",
    "uv sync --extra network-explorer\n",
    "\n",
    "# Run the dashboard\n",
    "uv run python src/visualize/transaction_network_explorer/dashboard.py --experiment tutorial_demo\n",
    "```\n",
    "\n",
    "Then open http://localhost:5006 in your browser.\n",
    "\n",
    "The dashboard provides:\n",
    "- **Bank Filtering**: Select which banks to include\n",
    "- **Transaction Model Filtering**: Choose specific laundering and legitimate patterns\n",
    "- **Time Range Selection**: Focus on specific time periods\n",
    "- **Multiple Visualizations**: Amount distributions, degree distributions, and temporal patterns\n",
    "\n",
    "All visualizations update in real-time as you adjust the filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed a full workflow through the AMLGentex framework:\n",
    "\n",
    "✅ **Created** an experiment with minimal configuration  \n",
    "✅ **Generated** synthetic AML transaction data  \n",
    "✅ **Preprocessed** transactions into ML-ready features  \n",
    "✅ **Trained** models in three settings (centralized, federated, isolated)  \n",
    "✅ **Visualized** and compared results  \n",
    "✅ **Explored** transactions with interactive widgets  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Convention over Configuration**: By following standard directory structure, the framework auto-discovers paths and clients\n",
    "2. **Privacy-Preserving Learning**: Federated learning enables collaboration without sharing raw data\n",
    "3. **Flexible Architecture**: Easy to compare different training paradigms\n",
    "4. **Scalable**: Framework handles small demos to large-scale experiments\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Experiment with different models**: Try GCN, GAT, MLP, or tree-based models\n",
    "- **Optimize hyperparameters**: Use `scripts/tune_hyperparams.py` for Bayesian optimization\n",
    "- **Scale up**: Create larger experiments with more accounts and banks\n",
    "- **Customize patterns**: Modify SAR transaction patterns in config files\n",
    "- **Analyze results**: Use the visualization tools to understand model behavior\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- **ML Module**: `src/ml/README.md`\n",
    "- **Scripts**: `scripts/README.md`\n",
    "- **Project**: `README.md`\n",
    "\n",
    "### Command-Line Usage\n",
    "\n",
    "All steps can also be run from the command line:\n",
    "\n",
    "```bash\n",
    "# Generate data\n",
    "python scripts/generate.py --conf_file experiments/tutorial_demo/config/data.yaml\n",
    "\n",
    "# Preprocess\n",
    "python scripts/preprocess.py --config experiments/tutorial_demo/config/preprocessing.yaml\n",
    "\n",
    "# Train (centralized)\n",
    "python -m src.ml.training.centralized \\\n",
    "  --config experiments/tutorial_demo/config/models.yaml \\\n",
    "  --model_type GraphSAGE \\\n",
    "  --client_type TorchGeometricClient\n",
    "\n",
    "# Visualize\n",
    "python scripts/plot.py --experiment tutorial_demo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
